{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory and nvprof</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated codebases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the **NVIDIA Command Line Profiler** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the **NVIDIA Command Line Profiler** (**nprof**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nvprof` is the NVIDIA command line profiler. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nvprof` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nvprof` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA codebases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: Profile an Application with nvprof\n",
    "\n",
    "[01-vector-add.cu](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (by `CTRL` + clicking them). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nvprof`.\n",
    "\n",
    "After profiling the application, answer the following questions using information displayed in the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_70 -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==190== NVPROF is profiling process 190, command: ./single-thread-vector-add\n",
      "Success! All values calculated correctly.\n",
      "==190== Profiling application: ./single-thread-vector-add\n",
      "==190== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  2.37084s         1  2.37084s  2.37084s  2.37084s  addVectorsInto(float*, float*, float*, int)\n",
      "      API calls:   71.42%  2.37087s         1  2.37087s  2.37087s  2.37087s  cudaDeviceSynchronize\n",
      "                   27.84%  924.26ms         3  308.09ms  19.116us  924.20ms  cudaMallocManaged\n",
      "                    0.71%  23.596ms         3  7.8652ms  7.2328ms  9.0417ms  cudaFree\n",
      "                    0.01%  303.67us        94  3.2300us     609ns  115.52us  cuDeviceGetAttribute\n",
      "                    0.01%  249.53us         1  249.53us  249.53us  249.53us  cuDeviceTotalMem\n",
      "                    0.00%  124.81us         1  124.81us  124.81us  124.81us  cudaLaunch\n",
      "                    0.00%  18.655us         1  18.655us  18.655us  18.655us  cuDeviceGetName\n",
      "                    0.00%  8.4200us         4  2.1050us     696ns  5.7630us  cudaSetupArgument\n",
      "                    0.00%  5.2850us         1  5.2850us  5.2850us  5.2850us  cudaConfigureCall\n",
      "                    0.00%  3.7180us         3  1.2390us     626ns  1.8670us  cuDeviceGetCount\n",
      "                    0.00%  1.8720us         2     936ns     703ns  1.1690us  cuDeviceGet\n",
      "                    0.00%  1.0300us         1  1.0300us  1.0300us  1.0300us  cudaGetLastError\n",
      "\n",
      "==190== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    2304  170.67KB  4.0000KB  0.9961MB  384.0000MB  42.15408ms  Host To Device\n",
      "     768  170.67KB  4.0000KB  0.9961MB  128.0000MB  11.29107ms  Device To Host\n",
      "     768         -         -         -           -  113.5584ms  Gpu page fault groups\n",
      "Total CPU Page faults: 1536\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nvprof` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_70 -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==244== NVPROF is profiling process 244, command: ./multi-thread-vector-add\n",
      "Success! All values calculated correctly.\n",
      "==244== Profiling application: ./multi-thread-vector-add\n",
      "==244== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  722.02ms         1  722.02ms  722.02ms  722.02ms  addVectorsInto(float*, float*, float*, int)\n",
      "      API calls:   79.93%  722.05ms         1  722.05ms  722.05ms  722.05ms  cudaDeviceSynchronize\n",
      "                   17.44%  157.58ms         3  52.528ms  19.074us  157.52ms  cudaMallocManaged\n",
      "                    2.56%  23.087ms         3  7.6955ms  7.0528ms  8.9298ms  cudaFree\n",
      "                    0.03%  256.11us        94  2.7240us     610ns  68.869us  cuDeviceGetAttribute\n",
      "                    0.03%  248.72us         1  248.72us  248.72us  248.72us  cuDeviceTotalMem\n",
      "                    0.01%  122.36us         1  122.36us  122.36us  122.36us  cudaLaunch\n",
      "                    0.00%  18.094us         1  18.094us  18.094us  18.094us  cuDeviceGetName\n",
      "                    0.00%  16.011us         4  4.0020us     661ns  7.6530us  cudaSetupArgument\n",
      "                    0.00%  4.2110us         3  1.4030us     664ns  2.3760us  cuDeviceGetCount\n",
      "                    0.00%  3.9300us         1  3.9300us  3.9300us  3.9300us  cudaConfigureCall\n",
      "                    0.00%  2.2040us         2  1.1020us     703ns  1.5010us  cuDeviceGet\n",
      "                    0.00%     941ns         1     941ns     941ns     941ns  cudaGetLastError\n",
      "\n",
      "==244== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    2304  170.67KB  4.0000KB  0.9961MB  384.0000MB  42.12250ms  Host To Device\n",
      "     768  170.67KB  4.0000KB  0.9961MB  128.0000MB  11.28970ms  Device To Host\n",
      "     768         -         -         -           -  113.1725ms  Gpu page fault groups\n",
      "Total CPU Page faults: 1536\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimzation you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_70 -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==460== NVPROF is profiling process 460, command: ./iteratively-optimized-vector-add\n",
      "Success! All values calculated correctly.\n",
      "==460== Profiling application: ./iteratively-optimized-vector-add\n",
      "==460== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  413.97ms         1  413.97ms  413.97ms  413.97ms  addVectorsInto(float*, float*, float*, int)\n",
      "      API calls:   69.45%  413.96ms         1  413.96ms  413.96ms  413.96ms  cudaDeviceSynchronize\n",
      "                   26.55%  158.24ms         3  52.748ms  18.999us  158.18ms  cudaMallocManaged\n",
      "                    3.90%  23.224ms         3  7.7413ms  7.0717ms  9.0250ms  cudaFree\n",
      "                    0.04%  256.17us        94  2.7250us     614ns  68.464us  cuDeviceGetAttribute\n",
      "                    0.04%  251.40us         1  251.40us  251.40us  251.40us  cuDeviceTotalMem\n",
      "                    0.02%  124.00us         1  124.00us  124.00us  124.00us  cudaLaunch\n",
      "                    0.00%  17.830us         1  17.830us  17.830us  17.830us  cuDeviceGetName\n",
      "                    0.00%  8.5500us         4  2.1370us     667ns  6.0700us  cudaSetupArgument\n",
      "                    0.00%  3.7820us         1  3.7820us  3.7820us  3.7820us  cudaConfigureCall\n",
      "                    0.00%  3.5950us         3  1.1980us     632ns  1.8660us  cuDeviceGetCount\n",
      "                    0.00%  1.8830us         2     941ns     710ns  1.1730us  cuDeviceGet\n",
      "                    0.00%     864ns         1     864ns     864ns     864ns  cudaGetLastError\n",
      "\n",
      "==460== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    2304  170.67KB  4.0000KB  0.9961MB  384.0000MB  42.15485ms  Host To Device\n",
      "     768  170.67KB  4.0000KB  0.9961MB  128.0000MB  11.28646ms  Device To Host\n",
      "     768         -         -         -           -  110.3101ms  Gpu page fault groups\n",
      "Total CPU Page faults: 1536\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQTzaK1iaFkxgYxaxR5QgHCVx1ZqhpX2F3q9UU6sGKCYaNIq6CGAo8W_qyzg2qwpeiZoHd7NCug7OTj/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQTzaK1iaFkxgYxaxR5QgHCVx1ZqhpX2F3q9UU6sGKCYaNIq6CGAo8W_qyzg2qwpeiZoHd7NCug7OTj/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a codebase. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [`01-get-device-properties.cu`](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [`01-get-device-properties.cu`](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\r\n",
      "Number of SMs: 80\r\n",
      "Compute Capability Major: 7\r\n",
      "Compute Capability Minor: 0\r\n",
      "Warp Size: 32\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_70 -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nvprof` so that you can quantitatively evaulate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_70 -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==618== NVPROF is profiling process 618, command: ./sm-optimized-vector-add\n",
      "Success! All values calculated correctly.\n",
      "==618== Profiling application: ./sm-optimized-vector-add\n",
      "==618== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  141.77ms         1  141.77ms  141.77ms  141.77ms  addVectorsInto(float*, float*, float*, int)\n",
      "      API calls:   49.47%  163.36ms         3  54.455ms  19.702us  163.30ms  cudaMallocManaged\n",
      "                   42.93%  141.76ms         1  141.76ms  141.76ms  141.76ms  cudaDeviceSynchronize\n",
      "                    7.32%  24.191ms         3  8.0635ms  7.4344ms  9.2131ms  cudaFree\n",
      "                    0.08%  257.65us        94  2.7400us     611ns  70.417us  cuDeviceGetAttribute\n",
      "                    0.08%  252.35us         1  252.35us  252.35us  252.35us  cudaGetDeviceProperties\n",
      "                    0.08%  250.09us         1  250.09us  250.09us  250.09us  cuDeviceTotalMem\n",
      "                    0.04%  134.22us         1  134.22us  134.22us  134.22us  cudaLaunch\n",
      "                    0.01%  19.020us         1  19.020us  19.020us  19.020us  cuDeviceGetName\n",
      "                    0.00%  6.0610us         1  6.0610us  6.0610us  6.0610us  cudaGetDevice\n",
      "                    0.00%  3.8320us         3  1.2770us     635ns  1.9600us  cuDeviceGetCount\n",
      "                    0.00%  3.5400us         4     885ns     741ns  1.0310us  cudaSetupArgument\n",
      "                    0.00%  3.4320us         1  3.4320us  3.4320us  3.4320us  cudaConfigureCall\n",
      "                    0.00%  1.9000us         2     950ns     725ns  1.1750us  cuDeviceGet\n",
      "                    0.00%     956ns         1     956ns     956ns     956ns  cudaGetLastError\n",
      "\n",
      "==618== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    2304  170.67KB  4.0000KB  0.9961MB  384.0000MB  42.21629ms  Host To Device\n",
      "     768  170.67KB  4.0000KB  0.9961MB  128.0000MB  11.32691ms  Device To Host\n",
      "     772         -         -         -           -  125.6571ms  Gpu page fault groups\n",
      "Total CPU Page faults: 1536\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocting memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works. `nvprof` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vS0-BCGiWUb82r1RH-4cSRmZjN2vjebqoodlHIN1fvtt1iDh8X8W9WOSlLVxcsY747WVIebw13cDYBO/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vS0-BCGiWUb82r1RH-4cSRmZjN2vjebqoodlHIN1fvtt1iDh8X8W9WOSlLVxcsY747WVIebw13cDYBO/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Page Faulting\n",
    "\n",
    "`nvprof` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nvprof`'s Unified Memory output section after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[`01-page-faults.cu`](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Curently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [`01-page-faults.cu`](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the codebase, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nvprof` output, specifically CPU and GPU page faults, for each of the 4 experiments you are conducting. There are links to solutions for each of the 4 experiments which you can refer to if you get stuck.\n",
    "\n",
    "- What happens when unified memory is accessed only by the CPU? ([solution](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- What happens when unified memory is accessed only by the GPU? ([solution](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- What happens when unified memory is accessed first by the CPU then the GPU? ([solution](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- What happens when unified memory is accessed first by the GPU then the CPU? ([solution](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==834== NVPROF is profiling process 834, command: ./page-faults\n",
      "==834== Profiling application: ./page-faults\n",
      "==834== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  51.636ms         1  51.636ms  51.636ms  51.636ms  deviceKernel(int*, int)\n",
      "      API calls:   72.15%  158.19ms         1  158.19ms  158.19ms  158.19ms  cudaMallocManaged\n",
      "                   23.56%  51.644ms         1  51.644ms  51.644ms  51.644ms  cudaDeviceSynchronize\n",
      "                    4.00%  8.7639ms         1  8.7639ms  8.7639ms  8.7639ms  cudaFree\n",
      "                    0.12%  260.79us        94  2.7740us     640ns  70.319us  cuDeviceGetAttribute\n",
      "                    0.11%  251.23us         1  251.23us  251.23us  251.23us  cuDeviceTotalMem\n",
      "                    0.05%  101.34us         1  101.34us  101.34us  101.34us  cudaLaunch\n",
      "                    0.01%  19.950us         1  19.950us  19.950us  19.950us  cuDeviceGetName\n",
      "                    0.00%  4.5140us         2  2.2570us     776ns  3.7380us  cudaSetupArgument\n",
      "                    0.00%  3.8810us         3  1.2930us     662ns  1.9170us  cuDeviceGetCount\n",
      "                    0.00%  2.6080us         1  2.6080us  2.6080us  2.6080us  cudaConfigureCall\n",
      "                    0.00%  1.9780us         2     989ns     720ns  1.2580us  cuDeviceGet\n",
      "\n",
      "==834== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "     768  170.67KB  4.0000KB  0.9961MB  128.0000MB  11.40275ms  Device To Host\n",
      "     383         -         -         -           -  51.16646ms  Gpu page fault groups\n",
      "Total CPU Page faults: 384\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the codebase in its current state, and hypothesize about what kinds of page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the Unified Memory section of the profiler output. Can you explain the page faulting descriptions based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==846== NVPROF is profiling process 846, command: ./sm-optimized-vector-add\n",
      "Success! All values calculated correctly.\n",
      "==846== Profiling application: ./sm-optimized-vector-add\n",
      "==846== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  143.63ms         1  143.63ms  143.63ms  143.63ms  addVectorsInto(float*, float*, float*, int)\n",
      "      API calls:   48.41%  158.23ms         3  52.743ms  18.333us  158.17ms  cudaMallocManaged\n",
      "                   43.95%  143.64ms         1  143.64ms  143.64ms  143.64ms  cudaDeviceSynchronize\n",
      "                    7.35%  24.024ms         3  8.0081ms  7.3212ms  9.1470ms  cudaFree\n",
      "                    0.08%  256.39us        94  2.7270us     613ns  68.706us  cuDeviceGetAttribute\n",
      "                    0.08%  250.67us         1  250.67us  250.67us  250.67us  cuDeviceTotalMem\n",
      "                    0.08%  250.05us         1  250.05us  250.05us  250.05us  cudaGetDeviceProperties\n",
      "                    0.04%  122.30us         1  122.30us  122.30us  122.30us  cudaLaunch\n",
      "                    0.01%  17.947us         1  17.947us  17.947us  17.947us  cuDeviceGetName\n",
      "                    0.00%  9.0590us         1  9.0590us  9.0590us  9.0590us  cudaConfigureCall\n",
      "                    0.00%  6.2470us         1  6.2470us  6.2470us  6.2470us  cudaGetDevice\n",
      "                    0.00%  3.9810us         3  1.3270us     636ns  2.2230us  cuDeviceGetCount\n",
      "                    0.00%  3.6500us         4     912ns     701ns  1.1280us  cudaSetupArgument\n",
      "                    0.00%  1.8760us         2     938ns     697ns  1.1790us  cuDeviceGet\n",
      "                    0.00%     843ns         1     843ns     843ns     843ns  cudaGetLastError\n",
      "\n",
      "==846== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    2304  170.67KB  4.0000KB  0.9961MB  384.0000MB  42.23213ms  Host To Device\n",
      "     768  170.67KB  4.0000KB  0.9961MB  128.0000MB  11.34787ms  Device To Host\n",
      "     772         -         -         -           -  126.9414ms  Gpu page fault groups\n",
      "Total CPU Page faults: 1536\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nvprof` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM page-fault behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_70 -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1008== NVPROF is profiling process 1008, command: ./initialize-in-kernel\n",
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n",
      "==1008== Profiling application: ./initialize-in-kernel\n",
      "==1008== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   99.00%  49.838ms         3  16.613ms  16.174ms  16.924ms  initWith(float, float*, int)\n",
      "                    1.00%  504.18us         1  504.18us  504.18us  504.18us  addArraysInto(float*, float*, float*, int)\n",
      "      API calls:   69.01%  157.96ms         3  52.654ms  19.111us  157.91ms  cudaMallocManaged\n",
      "                   21.95%  50.254ms         2  25.127ms  505.80us  49.748ms  cudaDeviceSynchronize\n",
      "                    8.60%  19.687ms         3  6.5622ms  5.5125ms  8.6397ms  cudaFree\n",
      "                    0.16%  375.41us         4  93.853us  8.6520us  251.72us  cudaLaunch\n",
      "                    0.12%  263.79us        94  2.8060us     610ns  70.476us  cuDeviceGetAttribute\n",
      "                    0.11%  249.94us         1  249.94us  249.94us  249.94us  cuDeviceTotalMem\n",
      "                    0.03%  58.329us         4  14.582us     729ns  55.132us  cudaConfigureCall\n",
      "                    0.01%  18.161us         1  18.161us  18.161us  18.161us  cuDeviceGetName\n",
      "                    0.01%  13.637us        13  1.0490us     593ns  4.7110us  cudaSetupArgument\n",
      "                    0.01%  11.857us         1  11.857us  11.857us  11.857us  cudaGetDevice\n",
      "                    0.00%  3.9570us         3  1.3190us     649ns  2.2000us  cuDeviceGetCount\n",
      "                    0.00%  1.9570us         2     978ns     726ns  1.2310us  cuDeviceGet\n",
      "                    0.00%  1.3130us         1  1.3130us  1.3130us  1.3130us  cudaDeviceGetAttribute\n",
      "                    0.00%     945ns         1     945ns     945ns     945ns  cudaGetLastError\n",
      "\n",
      "==1008== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "     768  170.67KB  4.0000KB  0.9961MB  128.0000MB  11.37693ms  Device To Host\n",
      "     243         -         -         -           -  49.39629ms  Gpu page fault groups\n",
      "Total CPU Page faults: 384\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/08-prefetch/01-vector-add-prefetch.cu), and update your own codebase to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specificially, as well as the impact on the reported run time of the initialization kernel, before each experiement, and then verify by running `nvprof`. Refer to [the solution](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_70 -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1173== NVPROF is profiling process 1173, command: ./prefetch-to-gpu\n",
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n",
      "==1173== Profiling application: ./prefetch-to-gpu\n",
      "==1173== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   51.66%  503.94us         1  503.94us  503.94us  503.94us  addArraysInto(float*, float*, float*, int)\n",
      "                   48.34%  471.50us         3  157.17us  154.69us  160.96us  initWith(float, float*, int)\n",
      "      API calls:   82.02%  157.82ms         3  52.607ms  21.527us  157.76ms  cudaMallocManaged\n",
      "                    9.86%  18.964ms         3  6.3213ms  5.1780ms  8.5974ms  cudaFree\n",
      "                    4.93%  9.4830ms         2  4.7415ms  505.43us  8.9775ms  cudaDeviceSynchronize\n",
      "                    2.79%  5.3624ms         3  1.7875ms  13.676us  4.7890ms  cudaMemPrefetchAsync\n",
      "                    0.13%  256.14us        94  2.7240us     615ns  69.254us  cuDeviceGetAttribute\n",
      "                    0.13%  248.80us         1  248.80us  248.80us  248.80us  cuDeviceTotalMem\n",
      "                    0.12%  225.44us         4  56.360us  8.2150us  113.99us  cudaLaunch\n",
      "                    0.01%  17.786us         1  17.786us  17.786us  17.786us  cuDeviceGetName\n",
      "                    0.01%  12.538us        13     964ns     617ns  3.8980us  cudaSetupArgument\n",
      "                    0.00%  9.5040us         1  9.5040us  9.5040us  9.5040us  cudaGetDevice\n",
      "                    0.00%  4.1500us         4  1.0370us     881ns  1.2820us  cudaConfigureCall\n",
      "                    0.00%  3.9200us         3  1.3060us     679ns  2.1380us  cuDeviceGetCount\n",
      "                    0.00%  1.6880us         2     844ns     683ns  1.0050us  cuDeviceGet\n",
      "                    0.00%  1.2860us         1  1.2860us  1.2860us  1.2860us  cudaDeviceGetAttribute\n",
      "                    0.00%  1.0050us         1  1.0050us  1.0050us  1.0050us  cudaGetLastError\n",
      "\n",
      "==1173== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "     768  170.67KB  4.0000KB  0.9961MB  128.0000MB  11.36883ms  Device To Host\n",
      "Total CPU Page faults: 384\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nvprof` to confirm. Refer to [the solution](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_70 -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1229== NVPROF is profiling process 1229, command: ./prefetch-to-cpu\n",
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n",
      "==1229== Profiling application: ./prefetch-to-cpu\n",
      "==1229== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   51.51%  502.69us         1  502.69us  502.69us  502.69us  addArraysInto(float*, float*, float*, int)\n",
      "                   48.49%  473.28us         3  157.76us  155.58us  162.05us  initWith(float, float*, int)\n",
      "      API calls:   75.11%  158.31ms         3  52.771ms  21.526us  158.25ms  cudaMallocManaged\n",
      "                   10.45%  22.036ms         4  5.5090ms  13.354us  18.481ms  cudaMemPrefetchAsync\n",
      "                    8.85%  18.657ms         3  6.2189ms  5.2277ms  8.1403ms  cudaFree\n",
      "                    5.21%  10.982ms         2  5.4908ms  504.94us  10.477ms  cudaDeviceSynchronize\n",
      "                    0.14%  285.71us         1  285.71us  285.71us  285.71us  cuDeviceTotalMem\n",
      "                    0.12%  257.55us        94  2.7390us     611ns  69.550us  cuDeviceGetAttribute\n",
      "                    0.10%  201.71us         4  50.428us  8.0960us  100.05us  cudaLaunch\n",
      "                    0.01%  17.560us         1  17.560us  17.560us  17.560us  cuDeviceGetName\n",
      "                    0.01%  11.880us        13     913ns     599ns  3.6610us  cudaSetupArgument\n",
      "                    0.00%  8.5220us         1  8.5220us  8.5220us  8.5220us  cudaGetDevice\n",
      "                    0.00%  4.5480us         4  1.1370us     755ns  1.7590us  cudaConfigureCall\n",
      "                    0.00%  3.4430us         3  1.1470us     612ns  1.8710us  cuDeviceGetCount\n",
      "                    0.00%  1.6410us         2     820ns     677ns     964ns  cuDeviceGet\n",
      "                    0.00%  1.3300us         1  1.3300us  1.3300us  1.3300us  cudaDeviceGetAttribute\n",
      "                    0.00%     920ns         1     920ns     920ns     920ns  cudaGetLastError\n",
      "\n",
      "==1229== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "      64  2.0000MB  2.0000MB  2.0000MB  128.0000MB  10.64784ms  Device To Host\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the **NVIDIA Command Line Profiler** (**nvprof**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated [SAXPY](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_1) application has been provided for you [here](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/09-saxpy/01-saxpy.cu). It currently contains a couple of bugs that you will need to find and fix before you can successfully compile, run, and then profile it with `nvprof`.\n",
    "\n",
    "After fixing the bugs and profiling the application, record the runtime of the `saxpy` kernel and then work *iteratively* to optimize the application, using `nvprof` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *50us*. Check out [the solution](../../../../../edit/tasks/task1/task/02_AC_UM_NVPROF/09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \r\n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_70 -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1297== NVPROF is profiling process 1297, command: ./saxpy\n",
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n",
      "==1297== Profiling application: ./saxpy\n",
      "==1297== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  71.914us         1  71.914us  71.914us  71.914us  saxpy(int*, int*, int*)\n",
      "      API calls:   94.31%  162.42ms         3  54.141ms  26.128us  162.36ms  cudaMallocManaged\n",
      "                    2.40%  4.1303ms         1  4.1303ms  4.1303ms  4.1303ms  cudaDeviceSynchronize\n",
      "                    1.58%  2.7279ms         3  909.29us  894.37us  928.88us  cudaFree\n",
      "                    1.31%  2.2584ms         3  752.79us  16.648us  2.0987ms  cudaMemPrefetchAsync\n",
      "                    0.15%  266.63us        94  2.8360us     614ns  70.172us  cuDeviceGetAttribute\n",
      "                    0.14%  249.00us         1  249.00us  249.00us  249.00us  cuDeviceTotalMem\n",
      "                    0.07%  126.25us         1  126.25us  126.25us  126.25us  cudaLaunch\n",
      "                    0.01%  18.176us         1  18.176us  18.176us  18.176us  cuDeviceGetName\n",
      "                    0.01%  9.9770us         1  9.9770us  9.9770us  9.9770us  cudaGetDevice\n",
      "                    0.00%  7.5000us         3  2.5000us     699ns  5.8840us  cudaSetupArgument\n",
      "                    0.00%  3.4390us         3  1.1460us     623ns  1.7110us  cuDeviceGetCount\n",
      "                    0.00%  1.8920us         2     946ns     695ns  1.1970us  cuDeviceGet\n",
      "                    0.00%  1.7990us         1  1.7990us  1.7990us  1.7990us  cudaConfigureCall\n",
      "                    0.00%  1.2360us         1  1.2360us  1.2360us  1.2360us  cudaDeviceGetAttribute\n",
      "\n",
      "==1297== Unified Memory profiling result:\n",
      "Device \"Tesla V100-SXM2-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "      24  2.0000MB  2.0000MB  2.0000MB  48.00000MB  4.678496ms  Host To Device\n",
      "       4  32.000KB  4.0000KB  60.000KB  128.0000KB  17.37600us  Device To Host\n",
      "Total CPU Page faults: 146\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
